{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23f64f82",
   "metadata": {},
   "source": [
    "## Volatility and Risk â€“ Median Shortfall or Value at Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c628ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#time period\n",
    "start = '2006-01-01'\n",
    "end = '2020-12-31'\n",
    "\n",
    "# Credit risk regimes\n",
    "#start = '2007-07-01'\n",
    "#end = '2010-04-01'\n",
    "\n",
    "# Covid-19 regimes\n",
    "#start = '2019-01-01'\n",
    "#end = '2020-12-31'\n",
    "\n",
    "# List of assets\n",
    "assets = ['^GSPC','^RUT','^GDAXI']\n",
    "\n",
    "# Downloading the data\n",
    "data = yf.download(assets, start = start, end = end)\n",
    "data = data.loc[:,('Adj Close', slice(None))]\n",
    "data.columns = assets\n",
    "ret = data.pct_change(1).dropna()\n",
    "\n",
    "ret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9582032",
   "metadata": {},
   "source": [
    "#### Exploring the return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec3cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret.kurt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(len(assets),1,figsize=(16, 16))\n",
    "\n",
    "for i in range(len(assets)):\n",
    "    ax[i].plot(ret[assets[i]], label=assets[i])\n",
    "    ax[i].legend()\n",
    "    ax[i].set_title('Daily returns of ' + assets[i])\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c2a0d",
   "metadata": {},
   "source": [
    "### VaR and Median Shortfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33fa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_at_risk(returns, confidence_level):\n",
    "    \"\"\"\n",
    "    Compute the Value-at-Risk metric of returns at confidence_level\n",
    "    :param returns: DataFrame\n",
    "    :param confidence_level: float\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the highest return in the lowest quantile (based on confidence level)\n",
    "    var = returns.quantile(q=confidence_level, interpolation=\"higher\")\n",
    "    return var\n",
    "\n",
    "def median_shortfall(returns, confidence_level):\n",
    "    \"\"\"\n",
    "    Compute the Median Shortfall metric of returns at confidence_level\n",
    "    :param returns: DataFrame\n",
    "    :param confidence_level: float\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the VaR of the returns\n",
    "    var = value_at_risk(returns, confidence_level)\n",
    "\n",
    "    # Find all returns in the worst quantile\n",
    "    worst_returns = returns[returns.lt(var)]\n",
    "\n",
    "    # Calculate median of all the worst returns\n",
    "    median_shortfall = worst_returns.median()\n",
    "\n",
    "    return median_shortfall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc31823",
   "metadata": {},
   "source": [
    "### VaR with GJR-GARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225f65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arch\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def get_var(returns, confidence_level):\n",
    "    #garch_model = arch.arch_model(returns, vol='Garch', p=1, q=1, dist='normal')\n",
    "    garch_model = arch.arch_model(returns, p = 1, q = 1, o = 1, mean = 'constant', vol = 'GARCH')\n",
    "    garch_fitted = garch_model.fit()\n",
    "    forecasts = garch_fitted.forecast(reindex=False)\n",
    "    cond_mean = forecasts.mean\n",
    "    cond_var = forecasts.variance\n",
    "    gm_resid = garch_fitted.resid\n",
    "    gm_std = garch_fitted.conditional_volatility\n",
    "    # Calculate the standardized residuals \n",
    "    gm_std_resid = gm_resid /gm_std\n",
    "    # Obtain the empirical quantiles\n",
    "    q = gm_std_resid.quantile(confidence_level)\n",
    "    \n",
    "    var = cond_mean.values + np.sqrt(cond_var).values * q\n",
    "    \n",
    "    return var[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca164b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ret.shape[0]\n",
    "\n",
    "Risk_hist = {}\n",
    "for i in assets:\n",
    "    Risk_hist[i] = {'VaR':[], 'MS':[], 'VaR_GJR-GARCH':[]}\n",
    "\n",
    "window = 250 \n",
    "confidence_level = 0.05\n",
    "\n",
    "for j in assets:\n",
    "    for i in range(window, n):\n",
    "        X = ret[j].iloc[i-window:i]\n",
    "        var_value = value_at_risk(X, confidence_level)\n",
    "        ms_value = median_shortfall(X, confidence_level)\n",
    "        garch_var = get_var(X, confidence_level)\n",
    "        \n",
    "        Risk_hist[j]['VaR'].append(var_value)\n",
    "        Risk_hist[j]['MS'].append(ms_value)\n",
    "        Risk_hist[j]['VaR_GJR-GARCH'].append(garch_var[0])\n",
    "            \n",
    "for i in assets:\n",
    "    Risk_hist[i] = pd.DataFrame(Risk_hist[i], index=ret.index[window:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a3ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(len(assets),1,figsize=(16, 16))\n",
    "\n",
    "for i in range(len(assets)):\n",
    "    ax[i].plot(ret[assets[i]].iloc[window:], label=assets[i])\n",
    "    ax[i].plot(Risk_hist[assets[i]]['VaR'], label=assets[i] + ' VaR')\n",
    "    ax[i].plot(Risk_hist[assets[i]]['MS'], label=assets[i] + ' MS')\n",
    "    ax[i].plot(Risk_hist[assets[i]]['VaR_GJR-GARCH'], label=assets[i] + ' VaR_GJR-GARCH')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_title('99% Backtesting Historal VaR, VaR_GJR-GARCH and MS of ' + assets[i])\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a057a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count number of exceedances\n",
    "for i in range(len(assets)):\n",
    "    varexceptions = (ret[assets[i]].iloc[window:] < Risk_hist[assets[i]]['VaR']).sum()\n",
    "    msexceptions = (ret[assets[i]].iloc[window:] < Risk_hist[assets[i]]['MS']).sum()\n",
    "    gvarexceptions = (ret[assets[i]].iloc[window:] < Risk_hist[assets[i]]['VaR_GJR-GARCH']).sum()\n",
    "    print(f\"For {assets[i]}: Number of VaR Exceptions: {varexceptions}\")\n",
    "    print(f\"For {assets[i]}: Number of MS Exceptions: {msexceptions}\")\n",
    "    print(f\"For {assets[i]}: Number of VaR_GJR-GARCH Exceptions: {gvarexceptions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5f34f",
   "metadata": {},
   "source": [
    "### Accuracy tests:\n",
    "\n",
    "##### Kupiec (1995) test\n",
    "\n",
    "Proportion of failure test for backtesting VaR and MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bern_test(p,v):\n",
    "    lv = len(v) #trading days\n",
    "    sv = sum(v) #number of failures\n",
    "    al = np.log(p)*sv + np.log(1-p)*(lv-sv)\n",
    "    bl = np.log(sv/lv)*sv + np.log(1-(sv/lv))*(lv-sv)\n",
    "    return (-2*(al-bl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae94a4",
   "metadata": {},
   "source": [
    "#### Christofferson (1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_test(v):\n",
    "    T = len(v)\n",
    "    J = np.full([T,4], 0)\n",
    "    for i in range(1,len(v)-1):\n",
    "        J[i,0] = (v[i-1] == 0) & (v[i] == 0)\n",
    "        J[i,1] = (v[i-1] == 0) & (v[i] == 1)\n",
    "        J[i,2] = (v[i-1] == 1) & (v[i] == 0)\n",
    "        J[i,3] = (v[i-1] == 1) & (v[i] == 1)\n",
    "    v_00 = sum(J[:,0])\n",
    "    v_01 = sum(J[:,1])\n",
    "    v_10 = sum(J[:,2])\n",
    "    v_11 = sum(J[:,3])\n",
    "    p_00=v_00/(v_00+v_01)\n",
    "    p_01=v_01/(v_00+v_01)\n",
    "    p_10=v_10/(v_10+v_11)\n",
    "    p_11=v_11/(v_10+v_11)\n",
    "    hat_p = (v_01+v_11)/(v_00+v_01+v_10+v_11)\n",
    "    al = np.log(1-hat_p)*(v_00+v_10) + np.log(hat_p)*(v_01+v_11)\n",
    "    bl = np.log(p_00)*v_00 + np.log(p_01)*v_01 + np.log(p_10)*v_10 + np.log(p_11)*v_11\n",
    "    return (-2*(al-bl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365345e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['Failure Ratio VaR','Failure Ratio MS','Failure Ratio VaR_GJR-GARCH','Bernoulli Test VaR stat','Bernoulli Test VaR p-value','Bernoulli Test MS stat',\n",
    "        'Bernoulli Test MS p-value','Bernoulli Test VaR_GJR-GARCH stat','Bernoulli Test VaR_GJR-GARCH p-value','Independent Test VaR stat',\n",
    "        'Independent Test VaR p-value','Independent Test MS stat','Independent Test MS p-value','Independent Test VaR_GJR-GARCH stat','Independent Test VaR_GJR-GARCH p-value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435846eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats_hist = {}\n",
    "\n",
    "for i in assets:\n",
    "    Stats_hist[i] = {}\n",
    "    for j in keys:\n",
    "        Stats_hist[i][j] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe95854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in Stats_hist.keys():\n",
    "    for j in Risk_hist[i].keys():\n",
    "        a = np.minimum(ret[i].iloc[window:] - Risk_hist[i][j], 0)\n",
    "        H = np.count_nonzero(a)\n",
    "        T = Risk_hist[i].shape[0]\n",
    "        q = a < 0\n",
    "        v = a * 0\n",
    "        v[q] = 1\n",
    "        ber = bern_test(confidence_level, v)\n",
    "        ind = ind_test(v)\n",
    "        Stats_hist[i]['Failure Ratio ' + j].append(H/T)\n",
    "        Stats_hist[i]['Bernoulli Test ' + j + ' stat'].append(round(ber, 5))\n",
    "        Stats_hist[i]['Bernoulli Test ' + j + ' p-value'].append(round(1 - st.chi2.cdf(ber, 1),5))\n",
    "        Stats_hist[i]['Independent Test ' + j + ' stat'].append(round(ind, 5))\n",
    "        Stats_hist[i]['Independent Test ' + j + ' p-value'].append(round(1 - st.chi2.cdf(ind, 1),5))\n",
    "        \n",
    "a = pd.DataFrame([])        \n",
    "for i in assets:\n",
    "    Stats_hist[i] = pd.DataFrame(Stats_hist[i]).T\n",
    "    Stats_hist[i].columns = [i]\n",
    "    a = pd.concat([a, Stats_hist[i]], axis=1)\n",
    "    \n",
    "Stats_hist = a\n",
    "display(Stats_hist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f296b3",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6388e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store DataFrames for each asset\n",
    "dfs = {}\n",
    "\n",
    "for asset in assets:\n",
    "    df1 = pd.DataFrame({'returns': ret[asset].iloc[window:], 'VaR': Risk_hist[asset]['VaR']})\n",
    "    df2 = pd.DataFrame({'returns': ret[asset].iloc[window:], 'MS': Risk_hist[asset]['MS']})\n",
    "    df3 = pd.DataFrame({'returns': ret[asset].iloc[window:], 'GJR-GARCH VaR': Risk_hist[asset]['VaR_GJR-GARCH']})\n",
    "    \n",
    "    # Store DataFrames in the dictionary with asset name as the key\n",
    "    dfs[asset] = {'df1': df1, 'df2': df2, 'df3': df3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lopez_rq_quadratic_loss(Returns, VaR):\n",
    "    \n",
    "    # Calculate quadratic loss for each time period based on Lopez's RQ\n",
    "    losses = np.where(Returns < VaR, 1 + (VaR - Returns) ** 2, 0)\n",
    "    \n",
    "    # Calculate the total quadratic loss\n",
    "    quadratic_loss_score = np.sum(losses)\n",
    "    \n",
    "    return quadratic_loss_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc162668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print quadratic loss scores for each asset\n",
    "for asset in assets:\n",
    "    print(f'Asset: {asset}')\n",
    "    \n",
    "    # Access the relevant DataFrames for the asset\n",
    "    df1 = dfs[asset]['df1']\n",
    "    df2 = dfs[asset]['df2']\n",
    "    df3 = dfs[asset]['df3']\n",
    "    \n",
    "    # Calculate and print quadratic loss scores for each DataFrame\n",
    "    score_df1 = lopez_rq_quadratic_loss(df1['returns'], df1['VaR'])\n",
    "    print(f'DF1 Quadratic Loss Score: {score_df1}')\n",
    "    \n",
    "    score_df2 = lopez_rq_quadratic_loss(df2['returns'], df2['MS'])\n",
    "    print(f'DF2 Quadratic Loss Score: {score_df2}')\n",
    "    \n",
    "    score_df3 = lopez_rq_quadratic_loss(df3['returns'], df3['GJR-GARCH VaR'])\n",
    "    print(f'DF3 Quadratic Loss Score: {score_df3}')\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba1343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caparonis_loss(Returns, VaR):\n",
    "    \n",
    "    # Calculate caporin loss for each time period \n",
    "    losses = np.where(Returns < VaR, abs(1 - abs(Returns/VaR)), 0)\n",
    "    \n",
    "    # Calculate the total caporin loss\n",
    "    caparonis_loss_score = np.sum(losses)\n",
    "    \n",
    "    return caparonis_loss_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print caporin loss scores for each asset\n",
    "for asset in assets:\n",
    "    print(f'Asset: {asset}')\n",
    "    \n",
    "    # Access the relevant DataFrames for the asset\n",
    "    df1 = dfs[asset]['df1']\n",
    "    df2 = dfs[asset]['df2']\n",
    "    df3 = dfs[asset]['df3']\n",
    "    \n",
    "    # Calculate and print Caporin loss scores for each DataFrame\n",
    "    score_df1 = caparonis_loss(df1['returns'], df1['VaR'])\n",
    "    print(f'DF1 Caporin Loss Score: {score_df1}')\n",
    "    \n",
    "    score_df2 = caparonis_loss(df2['returns'], df2['MS'])\n",
    "    print(f'DF2 Caporin Loss Score: {score_df2}')\n",
    "    \n",
    "    score_df3 = caparonis_loss(df3['returns'], df3['GJR-GARCH VaR'])\n",
    "    print(f'DF3 Caporin Loss Score: {score_df3}')\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb8422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
